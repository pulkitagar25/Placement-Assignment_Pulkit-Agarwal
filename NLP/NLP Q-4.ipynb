{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_from_file(file_path, num_sentences=3):\n",
    "    # Read the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stopwords_list = set(stopwords.words(\"english\") + list(punctuation))\n",
    "    words = [word for word in words if word not in stopwords_list]\n",
    "\n",
    "    # Calculate the frequency of each word\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "    # Assign scores to sentences based on word frequency\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                if len(sentence.split()) < 30:  # Consider sentences with less than 30 words\n",
    "                    if sentence not in sentence_scores:\n",
    "                        sentence_scores[sentence] = word_freq[word]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += word_freq[word]\n",
    "\n",
    "    # Select the top 'num_sentences' sentences with highest scores\n",
    "    summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These models are trained on massive amounts of text data and employ sophisticated algorithms, including deep neural networks, to learn the statistical properties of language and generate human-like text. Machine translation, another key area of NLP, utilizes data science techniques to automatically translate text from one language to another. With the abundance of digital text available, the application of data science techniques allows for the efficient processing and analysis of vast amounts of textual data. Data scientists employ various statistical, machine learning, and deep learning methods to uncover patterns, relationships, and insights hidden within text corpora. It encompasses a wide range of tasks, such as text classification, sentiment analysis, machine translation, information extraction, question answering, and text generation. By leveraging labeled datasets and supervised learning algorithms, data scientists can train models to automatically identify sentiments expressed in customer reviews, social media posts, and other forms of text. This extraction of structured data from unstructured text enables applications such as news aggregation, knowledge graph construction, and data mining. Text generation is an exciting application of NLP where data science plays a vital role. Sentiment analysis is another prominent area within NLP that heavily relies on data science techniques. One of the fundamental tasks in NLP is text classification, which involves assigning predefined categories or labels to pieces of text.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"D:/coding/Data Science/ineuron/Placement Assignment/NLP/Data Science and NLP.txt\"\n",
    "summary = generate_summary_from_file(file_path, num_sentences=10)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
